+++
draft = true
frontpic = ""
post = ""
tags = []
title = "Power of Hooks in Pytorch"

+++
## What are hooks?

Pytorch allows you to custom function calls to its **module** and **tensor** objects. The calls can both be added to the forward method of the object as well as the backward method. A hook added to the forward method will be called with the following arguments

1. The instance of the module itself
2. The input to the module
3. The output of the forward method

## Why hooks?

Now that we know what hooks are, it's important to understand their use case. Most commonly, they are either used for debugging purposes, calculating model size or calculating the number of ops. Let's say you imported a backbone from the **torchvision** package such as resnet18. If you wanted to calculate the number of ops for each layer you might be tempted to rewrite the entire resnet18 backbone with commands added to the forward method for calculating the ops. Instead a better way is to add a hook to the module without re-writing the code for resnet18.

## Example: Adding Dropout to a CNN

Let's demonstrate the power of hooks with an example of adding dropout after every conv2d layer of a CNN. Some knowledge of Python decorators is required to understand the code but other than it is quite simple. For example, This is the code for the ResNet module in the torchvision package. Needleesly to say adding dropout to the output of each layer is not as trivial.

### Adding the Hook

Let's write the hook that will do apply the dropout. The hook takes in 3 arguments i.e. the module itself, the input to the module and the output generated by forward method of the module. Our hook will just apply the dropout function to the output and overwrite it. The dropout2d arguments include the tensor to modify, the probability of dropping. The training flag can be set to be true only when the model is training or a custom combination of your choosing. Finally, inplace will overwrite the contents of output without creating a new tensor. This will raise an exception during training as autograd requires all outputs to be in memory for gradient propagation so we keep it as False.

\`\`\`

def dropout_hook(module, input, output):

    output = torch.nn.functional.dropout2d(output, p=0.5, training=module.training, inplace=False)

\`\`\`

Now that the hook is ready, we need to register it to the model itself. The way to do that is to call the \`\`\`register_forward_hook\`\`\` method of the module with the handle of the dropout_hook. This will add the dropout hook to every layer of the model. We only need to add this to the output of the convolutional layers. For that we create another function called register_hook

\`\`\`

def register_hook(self, module):

if isinstance(module, nn.Conv2d):

 module.register_forward_hook(dropout_hook)

model.apply(register_hook)

\`\`\`

The apply method is applied recursively to every nn.Module within the model so it is ensured that every conv2d layer will have the dropout hook added to it.

### Testing it Out

## References

1. [https://python-3-patterns-idioms-test.readthedocs.io/en/latest/PythonDecorators.html](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/PythonDecorators.html) "Decorators in Python3")
2. [http://funkyworklehead.blogspot.com/2008/12/how-to-decorate-init-or-another-python.html](http://funkyworklehead.blogspot.com/2008/12/how-to-decorate-init-or-another-python.html "Passing an object instance through a decorator")